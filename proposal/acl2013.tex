%
% File acl2013.tex
%
% Contact  navigli@di.uniroma1.it
%%
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to
\title{{\small CS 388 Natural Language Processing} \\ Project Proposal \\ {\small Due : April $7^{th}, 2014$}}
\author{Madhura Parikh \\
  {\tt mparikh@cs.utexas.edu} \\\And
  Avani Gupta\\
  {\tt avanigupta@utexas.edu} \\}
\date{}
\begin{document}
\maketitle
\section{Problem statetment}
We intend to work on the task of sentence similarity - i.e. given a pair of English sentences, we would like to score on a scale of $(0-5)$ how similar the two sentences are, with $5$ meaning semantically equivalent and $0$ meaning no similarity.  Specifically we shall be working on the Semantic Textual Similarity (STS) task - which was one of the $6$ tasks in Sem Eval 2012 and also the core task in *SEM 2013. Unlike binary decision, wherein pairwise sentences are considered to be either `similar' or `not similar',  this type of graded equivalence notion is likely to have much more utility, when applied to other NLP tasks like Machine Translation, Information Retrieval and Question Answering. One major motivation of choosing this for our class project is that it will mean interacting with several NLP areas we learnt in class - lemmatization, POS-tagging, parsing, WSD, etc to extract different measures of lexical, syntactic and semantic similarity. 
\section{Related Work}
One major advantage of working on the STS task is that there have been $80$ odd submissions for the official Sem Eval task in both 2012-13, meaning that we can draw from these experiences to find out which approaches and features worked the best and how we can leverage off them to get improved results.  A majority of the teams have used a supervised approach. For lexical similarity most teams have used the edit distance and other metrics of string similarity such the Needleman-Wunsch distance, the Smith-Waterman distance, etc. Most of the teams have also worked with the lemmatized sentences rather than considering the actual words. Most teams have also used POS-tags as a feature and some have also used chunking and chunk-overlap amongst the two sentences as well as n-gram and skip-gram overlap as features. Another often used feature is based on the dependency parse of the sentence, which enables comparing the S-V-O structure of sentences. For the n-gram modeling, several approaches have used not just the training data but also external corpora such as the Google n-gram 1T corpus, Wikipedia, etc. Another simple but  highly useful metric was to find the \emph{number overlap} , i.e if the numbers/dates/percentages that appear in one sentence match with those in the other. 

A much harder aspect of the problem is judging the semantic similarity of the two sentences. Most approaches first picked out the content words of each sentence and then used WordNet to find the similarity between the words pairwise, aggregating them in different ways to come up with a final similarity score. Interestingly while some teams used the least common ancestor or the shortest path metric, there really was no well defined metric that could be deemed helpful for the task - which is something we could look at.  The source used for similarity also played a very important role and one team mentions that using the Roget's thesaurus gave better results than WordNet. The BLEU measure was also a frequently used metric. LSA and Random Indexing were also commonly used as were several different distributional vector models. WSD was performed using the Lesk algorithm in conjunction with WordNet for most cases.  A few teams also used NER for judging similarity, while SRL was used just by a couple of teams. 

A few other interesting approaches used LDA based models to assign topics to a sentence, and used IR engines like Lucene as well as TF-IDF for comparing similarity. 

Most of the teams used SVR for predicting the final outcome and a couple of teams used an ensemble-based approach such as boosting.
\section{Our Contribution}
While we will be able to come up with a more concrete approach only once we start working with the data, there are a few contributions we hope to add. First of all we would like to use the features that were judged the best and most useful by a majority of the teams, using that as our baseline model. As improvisations on the baseline model, we suggest the following:
\begin{itemize}
\item Pinpoint which metric should be preferred when comparing similarity of word-pairs by traversing WordNet.
\item While teams have used dependency parse of a sentence, we would also like to use the syntactic Parse tree, and compute similarity using tree kernels
\item Another interesting approach that is frequently used in query engines is \emph{shingling} to determine how similar a query is to the target documents. We will incorporate this in our feature set.
\item Finally we would like to experiment which features work best for different classifiers and train these classifiers using different feature sets, using the ensemble as our final system.
\end{itemize} 
\section{Data}
We shall be using the testing and training data available from the Sem Eval 2012 and 2013 tasks for our project.
\section{Evaluation}
Just like the official Sem Eval STS task, we shall use the Pearson correlation coefficient as our primary evaluation metric.


\bibliographystyle{plain}
\nocite{*}
\bibliography{proposal.bib}


\end{document}
